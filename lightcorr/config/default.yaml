# General Dataset and Run Settings
model_type: "decision_tree"  # "decision_tree" or "random_forest"
dataset_path: "/home/yagnihotri/datasets/deepcorr_original_dataset"  # Path to the DeepCorr dataset
run_folder_path: "/home/yagnihotri/projects/corr/treecorr"  # Path to the folder where the run folder will be created
train_ratio: 0.8  # Training set ratio
flow_size: 300  # Flow size
negative_samples: 1  # Number of negative samples
load_all_data: false  # If true, load all data; otherwise, only load flows with minimum specified packets

# Hyperparameter Search Settings
hyperparameter_search_settings:
  hyperparameter_search_type: "none"  # Type of hyperparameter search ("none", "grid_search", "random_search")
  selected_hyperparameter_grid: "decision_tree_simple"  # Name of the hyperparameter grid to use (see below)

  grid_search:
    verbose: 2  # Verbosity level for hyperparameter search
    cv: 5  # Number of folds for cross-validation
    # Default parameters for GridSearchCV
    scoring: 'accuracy'  # Default scoring method
    n_jobs: -1  # Default number of jobs to run in parallel

  random_search:
    verbose: 2  # Verbosity level for hyperparameter search
    cv: 5  # Number of folds for cross-validation
    n_iter: 3  # Number of parameter settings sampled for random search
    # Default parameters for RandomizedSearchCV
    scoring: 'accuracy'  # Default scoring method
    n_jobs: -1  # Default number of jobs to run in parallel. -1 means all processors
    refit: True  # Default setting to refit an estimator
    pre_dispatch: '2*n_jobs'  # Default pre-dispatch configuration
    random_state: None  # Default random state
    error_score: 'raise'  # Default error handling
    return_train_score: False  # Default setting for returning train scores

# Hyperparameter Grid (For use with grid or random search)
hyperparameter_grid:
  decision_tree_extensive:
    criterion: ["gini", "entropy"] 
    splitter: ["best", "random"] 
    max_depth: [null, 10, 20, 30, 40, 50, 60]
    min_samples_split: [2, 4, 6, 8, 10] 
    min_samples_leaf: [1, 2, 3, 4, 5]  
    max_features: [null, "sqrt", "log2", 0.5, 0.75, 1.0]  
    class_weight: [null, "balanced", {0: 1, 1: 2}, {0: 1, 1: 3}]  
    min_impurity_decrease: [0.0, 0.01, 0.02, 0.05] 
    ccp_alpha: [0.0, 0.01, 0.02, 0.05] 

  decision_tree_simple:
    criterion: ["gini", "entropy"] 
    max_depth: [null, 10, 20, 30] 
    min_samples_split: [2, 5, 10]  
    min_samples_leaf: [1, 2, 4]  

# Single Model Training Settings (For training without hyperparameter search)
single_model_training:
  decision_tree:
    criterion: "gini"  # Function to measure the quality of a split
    splitter: "best"  # Strategy to choose the split at each node
    max_depth: null  # Maximum depth of the tree
    min_samples_split: 2  # Minimum number of samples required to split an internal node
    min_samples_leaf: 1  # Minimum number of samples required to be at a leaf node
    min_weight_fraction_leaf: 0.0  # Minimum weighted fraction of the sum total of weights required at a leaf node
    max_features: null  # Number of features to consider for the best split
    random_state: null  # Controls the randomness of the estimator
    max_leaf_nodes: null  # Maximum number of leaf nodes
    min_impurity_decrease: 0.0  # A node will be split if the decrease in impurity is greater than this value
    class_weight: null  # Weights associated with classes
    ccp_alpha: 0.0  # Complexity parameter used for Minimal Cost-Complexity Pruning