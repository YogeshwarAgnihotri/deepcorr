{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:05:33.750083Z",
     "start_time": "2023-10-26T14:05:33.619520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-26T14:05:53.273904Z",
     "start_time": "2023-10-26T14:05:48.077253Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "train? y\n"
     ]
    }
   ],
   "source": [
    "flow_size=300\n",
    "is_training=input('train?')\n",
    "TRAINING= True if is_training=='y' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_runs={'8872':'192.168.122.117','8802':'192.168.122.117','8873':'192.168.122.67','8803':'192.168.122.67',\n",
    "         '8874':'192.168.122.113','8804':'192.168.122.113','8875':'192.168.122.120',\n",
    "        '8876':'192.168.122.30','8877':'192.168.122.208','8878':'192.168.122.58'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "\n",
    "for name in all_runs:\n",
    "    dataset+=pickle.load(open('dataset/%s_tordata300.pickle'%name, 'rb'))\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "\n",
    "    len_tr=len(dataset)\n",
    "    train_ratio=float(len_tr-6000)/float(len_tr)\n",
    "    rr= list(range(len(dataset)))\n",
    "    np.random.shuffle(rr)\n",
    "\n",
    "    train_index=rr[:int(len_tr*train_ratio)]\n",
    "    test_index= rr[int(len_tr*train_ratio):] #range(len(dataset_test)) # #\n",
    "    pickle.dump(test_index,open('test_index300.pickle','wb'))\n",
    "else:\n",
    "    test_index=pickle.load(open('test_index300.pickle'))[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "negetive_samples=199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dataset,train_index,test_index,flow_size):\n",
    "    \n",
    "\n",
    "\n",
    "    global negetive_samples\n",
    "\n",
    "\n",
    "\n",
    "    all_samples=len(train_index)\n",
    "    labels=np.zeros((all_samples*(negetive_samples+1),1))\n",
    "    l2s=np.zeros((all_samples*(negetive_samples+1),8,flow_size,1))\n",
    "\n",
    "    index=0\n",
    "    random_ordering=[]+train_index\n",
    "    for i in tqdm.tqdm( train_index):\n",
    "        #[]#list(lsh.find_k_nearest_neighbors((Y_train[i]/ np.linalg.norm(Y_train[i])).astype(np.float64),(50)))\n",
    "\n",
    "        l2s[index,0,:,0]=np.array(dataset[i]['here'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "        l2s[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s[index,3,:,0]=np.array(dataset[i]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "        l2s[index,4,:,0]=np.array(dataset[i]['here'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "        l2s[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s[index,7,:,0]=np.array(dataset[i]['here'][1]['->'][:flow_size])/1000.0\n",
    "\n",
    "\n",
    "        if index % (negetive_samples+1) !=0:\n",
    "            print(index , len(nears))\n",
    "            raise\n",
    "        labels[index,0]=1\n",
    "        m=0\n",
    "        index+=1\n",
    "        np.random.shuffle(random_ordering)\n",
    "        for idx in random_ordering:\n",
    "            if idx==i or m>(negetive_samples-1):\n",
    "                continue\n",
    "\n",
    "            m+=1\n",
    "\n",
    "            l2s[index,0,:,0]=np.array(dataset[idx]['here'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "            l2s[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s[index,3,:,0]=np.array(dataset[idx]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "            l2s[index,4,:,0]=np.array(dataset[idx]['here'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "            l2s[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s[index,7,:,0]=np.array(dataset[idx]['here'][1]['->'][:flow_size])/1000.0\n",
    "\n",
    "            #l2s[index,0,:,0]=Y_train[i]#np.concatenate((Y_train[i],X_train[idx]))#(Y_train[i]*X_train[idx])/(np.linalg.norm(Y_train[i])*np.linalg.norm(X_train[idx]))\n",
    "            #l2s[index,1,:,0]=X_train[idx]\n",
    "\n",
    "\n",
    "\n",
    "            labels[index,0]=0\n",
    "            index+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #lsh.setup((X_test / np.linalg.norm(X_test,axis=1,keepdims=True)) .astype(np.float64))\n",
    "    index_hard=0\n",
    "    num_hard_test=0\n",
    "    l2s_test=np.zeros((len(test_index)*(negetive_samples+1),8,flow_size,1))\n",
    "    labels_test=np.zeros((len(test_index)*(negetive_samples+1)))\n",
    "    l2s_test_hard=np.zeros((num_hard_test*num_hard_test,2,flow_size,1))\n",
    "    index=0\n",
    "    random_test=[]+test_index\n",
    "\n",
    "    for i in tqdm.tqdm(test_index):\n",
    "        #list(lsh.find_k_nearest_neighbors((Y_test[i]/ np.linalg.norm(Y_test[i])).astype(np.float64),(50)))\n",
    "\n",
    "\n",
    "\n",
    "        if index % (negetive_samples+1) !=0:\n",
    "            print(index, nears)\n",
    "            raise \n",
    "        m=0\n",
    "\n",
    "        np.random.shuffle(random_test)\n",
    "        for idx in random_test:\n",
    "            if idx==i or m>(negetive_samples-1):\n",
    "                continue\n",
    "\n",
    "            m+=1\n",
    "            l2s_test[index,0,:,0]=np.array(dataset[idx]['here'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s_test[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "            l2s_test[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "            l2s_test[index,3,:,0]=np.array(dataset[idx]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "            l2s_test[index,4,:,0]=np.array(dataset[idx]['here'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s_test[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "            l2s_test[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "            l2s_test[index,7,:,0]=np.array(dataset[idx]['here'][1]['->'][:flow_size])/1000.0\n",
    "            labels_test[index]=0\n",
    "            index+=1\n",
    "\n",
    "        l2s_test[index,0,:,0]=np.array(dataset[i]['here'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s_test[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "        l2s_test[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "        l2s_test[index,3,:,0]=np.array(dataset[i]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "        l2s_test[index,4,:,0]=np.array(dataset[i]['here'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s_test[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "        l2s_test[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "        l2s_test[index,7,:,0]=np.array(dataset[i]['here'][1]['->'][:flow_size])/1000.0\n",
    "        #l2s_test[index,2,:,0]=dataset[i]['there'][0]['->'][:flow_size]\n",
    "        #l2s_test[index,3,:,0]=dataset[i]['here'][0]['<-'][:flow_size]\n",
    "\n",
    "        #l2s_test[index,0,:,1]=dataset[i]['here'][1]['->'][:flow_size]\n",
    "        #l2s_test[index,1,:,1]=dataset[i]['there'][1]['<-'][:flow_size]\n",
    "        #l2s_test[index,2,:,1]=dataset[i]['there'][1]['->'][:flow_size]\n",
    "        #l2s_test[index,3,:,1]=dataset[i]['here'][1]['<-'][:flow_size]\n",
    "        labels_test[index]=1\n",
    "\n",
    "        index+=1\n",
    "    return l2s, labels,l2s_test,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/uni/miniconda3/envs/deepcorr/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(flow_before,dropout_keep_prob):\n",
    "    last_layer=flow_before\n",
    "    flat_layers_after=[flow_size*2,1000,50,1]\n",
    "    for l in range(len(flat_layers_after)-1):\n",
    "        flat_weight = tf.get_variable(\"flat_after_weight%d\"%l, [flat_layers_after[l],flat_layers_after[l+1]],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.01,mean=0.0))\n",
    "\n",
    "        flat_bias = tf.get_variable(\"flat_after_bias%d\"%l, [flat_layers_after[l+1]],\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x=tf.add(\n",
    "                tf.matmul(last_layer, flat_weight),\n",
    "                flat_bias)\n",
    "        if l<len(flat_layers_after)-2:\n",
    "            _x=tf.nn.dropout(tf.nn.relu(_x,name='relu_noise_flat_%d'%l),keep_prob=dropout_keep_prob)\n",
    "        last_layer=_x\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(flow_before,dropout_keep_prob):\n",
    "    last_layer=flow_before\n",
    "    \n",
    "    CNN_LAYERS=[[2,20,1,2000,5],[4,10,2000,800,3]]\n",
    "    \n",
    "    for cnn_size in range(len(CNN_LAYERS)):\n",
    "        cnn_weights = tf.get_variable(\"cnn_weight%d\"%cnn_size, CNN_LAYERS[cnn_size][:-1],\n",
    "            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        cnn_bias = tf.get_variable(\"cnn_bias%d\"%cnn_size, [CNN_LAYERS[cnn_size][-2]],\n",
    "            initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x = tf.nn.conv2d(last_layer, cnn_weights, strides=[1, 2,2, 1], padding='VALID')\n",
    "        _x = tf.nn.bias_add(_x, cnn_bias)\n",
    "        conv = tf.nn.relu(_x,name='relu_cnn_%d'%cnn_size)\n",
    "        pool = tf.nn.max_pool(conv, ksize=[1, 1, CNN_LAYERS[cnn_size][-1], 1], strides=[1, 1, 1, 1],padding='VALID')\n",
    "        last_layer=pool\n",
    "    last_layer=tf.reshape(last_layer, [batch_size,-1])\n",
    "    \n",
    "    flat_layers_after=[49600,3000,800,100,1]\n",
    "    for l in range(len(flat_layers_after)-1):\n",
    "        flat_weight = tf.get_variable(\"flat_after_weight%d\"%l, [flat_layers_after[l],flat_layers_after[l+1]],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.01,mean=0.0))\n",
    "\n",
    "        flat_bias = tf.get_variable(\"flat_after_bias%d\"%l, [flat_layers_after[l+1]],\n",
    "        initializer=tf.zeros_initializer())\n",
    "\n",
    "        _x=tf.add(\n",
    "                tf.matmul(last_layer, flat_weight),\n",
    "                flat_bias)\n",
    "        if l<len(flat_layers_after)-2:\n",
    "            _x=tf.nn.dropout(tf.nn.relu(_x,name='relu_noise_flat_%d'%l),keep_prob=dropout_keep_prob)\n",
    "        last_layer=_x\n",
    "    return last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/uni/miniconda3/envs/deepcorr/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    batch_size=256\n",
    "    learn_rate=0.0001\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        train_flow_before = tf.placeholder(tf.float32, shape=[batch_size, 8,flow_size,1],name='flow_before_placeholder')\n",
    "        train_label = tf.placeholder(tf.float32,name='label_placeholder',shape=[batch_size,1])\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32,name='dropout_placeholder')\n",
    "        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n",
    "        # Look up embeddings for inputs.\n",
    "\n",
    "\n",
    "\n",
    "        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n",
    "        predict=tf.nn.sigmoid(y2)\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y2,labels=train_label),name='loss_sigmoid')\n",
    "\n",
    "\n",
    "        # tp = tf.contrib.metrics.streaming_true_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n",
    "        # fp = tf.contrib.metrics.streaming_false_positives(predictions=tf.nn.sigmoid(logits), labels=train_correlated)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "\n",
    "\n",
    "        #    gradients = tf.norm(tf.gradients(logits, weights['w_out']))\n",
    "\n",
    "        #    w_mean, w_var = tf.nn.moments(weights['w_out'], [0])\n",
    "        s_loss=tf.summary.scalar('loss', loss)\n",
    "        #    tf.summary.scalar('weight_norm', tf.norm(weights['w_out']))\n",
    "        #    tf.summary.scalar('weight_mean', tf.reduce_mean(w_mean))\n",
    "        #    tf.summary.scalar('weight_var', tf.reduce_mean(w_var))\n",
    "\n",
    "        #    tf.summary.scalar('bias', tf.reduce_mean(biases['b_out']))\n",
    "        #    tf.summary.scalar('logits', tf.reduce_mean(logits))\n",
    "        #    tf.summary.scalar('gradients', gradients)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "else:\n",
    "    batch_size=2804/2\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        train_flow_before = tf.placeholder(tf.float32, shape=[batch_size, 8,flow_size,1],name='flow_before_placeholder')\n",
    "        train_label = tf.placeholder(tf.float32,name='label_placeholder',shape=[batch_size,1])\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32,name='dropout_placeholder')\n",
    "        # train_correlated_var = tf.Variable(train_correlated, trainable=False)\n",
    "        # Look up embeddings for inputs.\n",
    "\n",
    "\n",
    "\n",
    "        y2 = model_cnn(train_flow_before, dropout_keep_prob)\n",
    "        predict=tf.nn.sigmoid(y2)\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1\n",
    "import datetime\n",
    "\n",
    "writer = tf.summary.FileWriter('./logs/tf_log/noise_classifier/allcir_300_'+str(datetime.datetime.now()), graph=graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 18:32:30.011595: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-10-26 18:32:30.019220: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1324/1324 [00:25<00:00, 51.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [02:17<00:00, 43.70it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Launch the graph\n",
    "# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "# with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "#saver = tf.train.Saver()\n",
    "if TRAINING:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        session.run(init)\n",
    "        \n",
    "\n",
    "        for epoch in range(num_epochs ):\n",
    "            l2s,labels,l2s_test,labels_test=generate_data(dataset=dataset,train_index=train_index,test_index=test_index,flow_size=flow_size)\n",
    "            rr= list(range(len(l2s)))\n",
    "            np.random.shuffle(rr)\n",
    "            l2s=l2s[rr]\n",
    "            labels=labels[rr]\n",
    "\n",
    "\n",
    "            average_loss = 0\n",
    "            new_epoch=True\n",
    "            num_steps= (len(l2s)//batch_size)-1\n",
    "\n",
    "            for step in range(num_steps):\n",
    "                start_ind = step*batch_size\n",
    "                end_ind = ((step + 1) *batch_size)\n",
    "                if end_ind < start_ind:\n",
    "                    print('HOOY')\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    batch_flow_before=l2s[start_ind:end_ind,:]\n",
    "                    batch_label= labels[start_ind:end_ind]\n",
    "\n",
    "\n",
    "                feed_dict = {train_flow_before: batch_flow_before,\n",
    "                                train_label:batch_label,\n",
    "                             dropout_keep_prob:0.6}\n",
    "                # We perform one update step by evaluating the optimizer op (including it\n",
    "                # in the list of returned values for session.run()\n",
    "\n",
    "                _, loss_val,summary = session.run([optimizer, loss, summary_op], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "\n",
    "                # average_loss += loss_val\n",
    "                writer.add_summary(summary, (epoch*num_steps) +step)\n",
    "\n",
    "                # print step, loss_val\n",
    "                # if step % FLAGS.print_every_n_steps == 0:\n",
    "                #     if step > 0:\n",
    "                #         average_loss /= FLAGS.print_every_n_steps\n",
    "                #     # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                #     print(\"Average loss at step \", step, \": \", average_loss)\n",
    "                #     average_loss = 0.\n",
    "\n",
    "                # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "\n",
    "                if ((epoch*num_steps) +step) % 100 == 0:\n",
    "                    print(\"Average loss on validation set at step \",  (epoch*num_steps) +step, \": \", loss_val)\n",
    "                if (((epoch*num_steps) +step)) % 3000 == 0 and epoch >1:\n",
    "                    tp=0\n",
    "                    fp=0\n",
    "\n",
    "                    num_steps_test= (len(l2s_test)//batch_size)-1\n",
    "                    Y_est=np.zeros((batch_size*(num_steps_test+1)))\n",
    "                    for step in range(num_steps_test):\n",
    "                        start_ind = step*batch_size\n",
    "                        end_ind = ((step + 1) *batch_size)\n",
    "                        test_batch_flow_before=l2s_test[start_ind:end_ind]\n",
    "                        feed_dict = {\n",
    "                                train_flow_before:test_batch_flow_before,\n",
    "                            dropout_keep_prob:1.0}\n",
    "\n",
    "\n",
    "                        est=session.run(predict, feed_dict=feed_dict)\n",
    "                        #est=np.array([xxx.sum() for xxx in test_batch_flow_before])\n",
    "                        Y_est[start_ind:end_ind]=est.reshape((batch_size))\n",
    "                    num_samples_test=len(l2s_test)/(negetive_samples+1)\n",
    "\n",
    "                    for idx in range(num_samples_test-1):\n",
    "                        best=np.argmax(Y_est[idx*(negetive_samples+1):(idx+1)*(negetive_samples+1)])\n",
    "\n",
    "                        if labels_test[best+(idx*(negetive_samples+1))]==1:\n",
    "                            tp+=1\n",
    "                        else:\n",
    "                            fp+=1\n",
    "                    print(tp,fp)\n",
    "                    acc= float(tp)/float(tp+fp)\n",
    "                    if float(tp)/float(tp+fp)>0.8:      \n",
    "                        print('saving...')\n",
    "                        save_path = saver.save(session, \"/mnt/nfs/work1/amir/milad/tor_199_epoch%d_step%d_acc%.2f.ckpt\"%(epoch,step,acc))\n",
    "                        print('saved')\n",
    "            print('Epoch',epoch)\n",
    "            #save_path = saver.save(session, \"/mnt/nfs/scratch1/milad/model_diff_large_1e4_epoch%d.ckpt\"%(epoch))\n",
    "\n",
    "            #t.join()\n",
    "else:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        name=input('model name')\n",
    "        saver.restore(session, \"/mnt/nfs/work1/amir/milad/%s\"%name)\n",
    "        print(\"Model restored.\")\n",
    "        corrs=np.zeros((len(test_index),len(test_index)))\n",
    "        batch=[]\n",
    "        l2s_test_all=np.zeros((batch_size,8,flow_size,1))\n",
    "        l_ids=[]\n",
    "        index=0\n",
    "        xi,xj=0,0\n",
    "        for i in tqdm.tqdm(test_index):\n",
    "            xj=0\n",
    "            for j in test_index:\n",
    "                \n",
    "                l2s_test_all[index,0,:,0]=np.array(dataset[j]['here'][0]['<-'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,1,:,0]=np.array(dataset[i]['there'][0]['->'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,2,:,0]=np.array(dataset[i]['there'][0]['<-'][:flow_size])*1000.0\n",
    "                l2s_test_all[index,3,:,0]=np.array(dataset[j]['here'][0]['->'][:flow_size])*1000.0\n",
    "\n",
    "                l2s_test_all[index,4,:,0]=np.array(dataset[j]['here'][1]['<-'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,5,:,0]=np.array(dataset[i]['there'][1]['->'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,6,:,0]=np.array(dataset[i]['there'][1]['<-'][:flow_size])/1000.0\n",
    "                l2s_test_all[index,7,:,0]=np.array(dataset[j]['here'][1]['->'][:flow_size])/1000.0\n",
    "                l_ids.append((xi,xj))\n",
    "                index+=1\n",
    "                if index==batch_size:\n",
    "                    index=0\n",
    "                    cor_vals=session.run(predict,feed_dict={train_flow_before:l2s_test_all,\n",
    "                            dropout_keep_prob:1.0})\n",
    "                    for ids in range(len(l_ids)):\n",
    "                        di,dj=l_ids[ids]\n",
    "                        corrs[di,dj]=cor_vals[ids]\n",
    "                    l_ids=[]\n",
    "                xj+=1\n",
    "            xi+=1\n",
    "        np.save(open('correlation_values_test.np','w'),corrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepcorr]",
   "language": "python",
   "name": "conda-env-deepcorr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
